import math
import os
from typing import Dict, Optional

import pandas
from pandas.core.frame import DataFrame
from pedestrians_video_2_carla.data.base.base_datamodule import BaseDataModule
from pedestrians_video_2_carla.data.openpose.constants import DF_USECOLS, DF_ISIN, JAAD_OPENPOSE_DIR
from pedestrians_video_2_carla.data.openpose.openpose_dataset import \
    OpenPoseDataset
from tqdm.std import tqdm


class JAADOpenPoseDataModule(BaseDataModule):
    def __init__(self,
                 df_usecols=DF_USECOLS,
                 df_isin: Optional[Dict] = DF_ISIN,
                 clip_offset: Optional[int] = 10,
                 openpose_dir: Optional[str] = JAAD_OPENPOSE_DIR,
                 **kwargs
                 ):
        self.annotations_usecols = df_usecols
        self.annotations_filters = df_isin
        self.clip_offset = clip_offset
        self.openpose_dir = openpose_dir

        self.__settings = {
            'clip_offset': self.clip_offset,
            'annotations_usecols': self.annotations_usecols,
            'annotations_filters': self.annotations_filters
        }

        super().__init__(**kwargs)

        # TODO: use original JAAD annotations instead of the generated by external script
        self.annotations_filepath = os.path.join(self.openpose_dir, 'annotations.csv')

    @property
    def settings(self):
        return {
            **super().settings,
            **self.__settings
        }

    @staticmethod
    def add_data_specific_args(parent_parser):
        BaseDataModule.add_data_specific_args(parent_parser)

        parser = parent_parser.add_argument_group('JAAD OpenPose DataModule')
        parser.add_argument(
            '--clip_offset',
            metavar='NUM_FRAMES',
            help='''
                Number of frames to shift from the BEGINNING of the last clip.
                Example: clip_length=30 and clip_offset=10 means that there will be
                20 frames overlap between subsequent clips.
                ''',
            type=int,
            default=10
        )
        parser.add_argument(
            '--openpose_dir',
            help="Directory where annotations.csv and OpenPose-extracted keypoints are stored.",
            type=str,
            default=JAAD_OPENPOSE_DIR
        )
        return parent_parser

    def prepare_data(self) -> None:
        # this is only called on one GPU, do not use self.something assignments

        if not self._needs_preparation:
            # we already have datasset prepared for this combination of settings
            return

        progress_bar = tqdm(total=8, desc='Generating subsets')

        # TODO: one day use JAAD annotations directly
        annotations_df: DataFrame = pandas.read_csv(
            self.annotations_filepath,
            usecols=self.annotations_usecols,
            index_col=['video', 'id']
        )

        progress_bar.update()

        filtering_results = annotations_df.isin(self.annotations_filters)[
            list(self.annotations_filters.keys())].all(axis=1)

        annotations_df = annotations_df[filtering_results]
        annotations_df.sort_index(inplace=True)

        # There is no 'senior' in CARLA, so replace with 'adult'
        annotations_df['age'] = annotations_df['age'].replace('senior', 'adult')

        progress_bar.update()

        frame_counts = annotations_df.groupby(['video', 'id']).agg(
            frame_count=pandas.NamedAgg(column="frame", aggfunc="count"),
            frame_min=pandas.NamedAgg(column="frame", aggfunc="min"),
            frame_max=pandas.NamedAgg(column="frame", aggfunc="max")
        ).assign(
            frame_diff=lambda x: x.frame_max - x.frame_min + 1
        ).assign(frame_count_eq_diff=lambda x: x.frame_count == x.frame_diff)

        # drop clips that are too short for sure
        frame_counts = frame_counts[frame_counts.frame_count >= self.clip_length]

        progress_bar.update()

        clips = []

        # handle continuous clips first
        for idx in frame_counts[frame_counts.frame_count_eq_diff == True].index:
            video = annotations_df.loc[idx]
            ci = 0
            while (ci*self.clip_offset + self.clip_length) <= frame_counts.loc[idx].frame_count:
                clips.append(video.iloc[ci * self.clip_offset:ci *
                                        self.clip_offset + self.clip_length].reset_index()[self.annotations_usecols].assign(clip=ci))
                ci += 1

        progress_bar.update()

        # then try to extract from non-continuos
        for idx in frame_counts[frame_counts.frame_count_eq_diff == False].index:
            video = annotations_df.loc[idx]
            frame_diffs_min = video[1:][['frame']].assign(
                frame_diff=video[1:].frame - video[0:-1].frame)
            frame_min = [frame_counts.loc[idx].frame_min] + \
                list(frame_diffs_min[frame_diffs_min.frame_diff > 1]['frame'].values)
            frame_diffs_max = video[0:-1][['frame']
                                          ].assign(frame_diff=video[1:].frame - video[0:-1].frame)
            frame_max = list(frame_diffs_max[frame_diffs_max.frame_diff > 1]
                             ['frame'].values) + [frame_counts.loc[idx].frame_max]
            ci = 0  # continuous for all clips
            for (fmin, fmax) in zip(frame_min, frame_max):
                while (ci*self.clip_offset + self.clip_length + fmin) <= fmax:
                    clips.append(video.loc[video.frame >= ci*self.clip_offset + fmin][:self.clip_length].reset_index()[
                        self.annotations_usecols].assign(clip=ci))
                    ci += 1

        progress_bar.update()

        self._split_clips(clips, ['video', 'id'], [
                          'clip', 'frame'], progress_bar=progress_bar, settings=self.__settings)

    def setup(self, stage: Optional[str] = None) -> None:
        return self._setup(dataset_creator=lambda *args, **kwargs: OpenPoseDataset(
            self.openpose_dir,
            *args, **kwargs
        ), stage=stage)
