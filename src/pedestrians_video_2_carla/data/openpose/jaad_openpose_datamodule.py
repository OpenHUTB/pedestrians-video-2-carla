import json
import os
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np
import pandas
from pandas.core.frame import DataFrame
from pedestrians_video_2_carla.data.base.base_datamodule import BaseDataModule
from pedestrians_video_2_carla.data.base.pandas_datamodule_mixin import PandasDataModuleMixin
from pedestrians_video_2_carla.data.openpose.constants import (DF_ISIN,
                                                               DF_USECOLS,
                                                               OPENPOSE_DIR)
from pedestrians_video_2_carla.data.openpose.openpose_dataset import \
    OpenPoseDataset
from tqdm.auto import tqdm


class JAADOpenPoseDataModule(PandasDataModuleMixin, BaseDataModule):
    def __init__(self,
                 df_usecols=DF_USECOLS,
                 df_filters: Optional[Dict] = DF_ISIN,
                 strong_points: Optional[float] = 0,
                 **kwargs
                 ):
        self.strong_points = strong_points

        # TODO: use original JAAD annotations instead of the generated by external script?
        super().__init__(
            data_filepath='annotations.csv',
            primary_index=['video', 'id'],
            clips_index=['clip', 'frame'],
            df_usecols=df_usecols,
            df_filters=df_filters,
            extra_cols={'keypoints': 'object'},
            **kwargs
        )

        self.openpose_dir = os.path.join(self.outputs_dir, OPENPOSE_DIR)

    @property
    def settings(self):
        return {
            **super().settings,
            'strong_points': self.strong_points,
        }

    @staticmethod
    def add_subclass_specific_args(parent_parser):
        parser = parent_parser.add_argument_group('JAAD OpenPose DataModule')
        parser.add_argument(
            '--strong_points',
            help='''
                Strong points threshold. If set to a value, only clips with
                with at least strong_points fraction of keypoints are used.
                ''',
            type=float,
            default=0
        )
        return parent_parser

    def _clean_filter_sort_data(self, annotations_df: DataFrame) -> DataFrame:
        # There is no 'senior' in CARLA, so replace with 'adult'
        annotations_df['age'].replace('senior', 'adult', inplace=True)

        return super()._clean_filter_sort_data(annotations_df)

    def _is_strong_points(self, clip: DataFrame) -> None:
        # TODO: use threshold instead of all or nothing?
        keypoints_list = clip.loc[:, 'keypoints'].tolist()
        keypoints = np.stack(keypoints_list)

        if self.strong_points < 1.0:
            return np.any(keypoints[..., :2], axis=-1).sum() >= self.strong_points * np.prod(keypoints.shape[:-1])
        else:
            return np.all(np.any(keypoints[..., :2], axis=-1))

    def _clean_filter_sort_clips(self, clips: List[DataFrame]) -> List[DataFrame]:
        if self.strong_points:
            # remove clips that contain missing data
            return [
                c
                for c in clips
                if self._is_strong_points(c)
            ]

        return clips

    def _get_raw_data(self, grouped: pandas.DataFrame) -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, Any]]:
        # projections
        projection_2d = self._reshape_to_sequences(grouped, 'keypoints')

        # targets
        bboxes = np.stack([
            self._reshape_to_sequences(grouped, 'x1'),
            self._reshape_to_sequences(grouped, 'y1'),
            self._reshape_to_sequences(grouped, 'x2'),
            self._reshape_to_sequences(grouped, 'y2'),
        ], axis=-1).astype(np.float32)

        targets = {
            'bboxes': bboxes.reshape((*bboxes.shape[:-1], 2, 2))
        }

        # meta
        grouped_head, grouped_tail = grouped.head(1).reset_index(
            drop=False), grouped.tail(1).reset_index(drop=False)
        meta = {
            'video_id': grouped_tail.loc[:, 'video'].to_list(),
            'pedestrian_id': grouped_tail.loc[:, 'id'].to_list(),
            'clip_id': grouped_tail.loc[:, 'clip'].to_numpy().astype(np.int32),
            'age': grouped_tail.loc[:, 'age'].to_list(),
            'gender': grouped_tail.loc[:, 'gender'].to_list(),
            'action': grouped_tail.loc[:, 'action'].to_list(),
            'speed': grouped_tail.loc[:, 'speed'].to_list(),
            'start_frame': grouped_head.loc[:, 'frame'].to_numpy().astype(np.int32),
            'end_frame': grouped_tail.loc[:, 'frame'].to_numpy().astype(np.int32) + 1,
        }

        return projection_2d, targets, meta

    def _get_dataset_creator(self) -> Callable:
        return OpenPoseDataset

    def _extract_additional_data(self, clips: List[DataFrame]):
        """
        Extract skeleton data from keypoint files. This potentially modifies data in place!

        :param clips: List of DataFrames
        :type clips: List[DataFrame]
        """
        updated_clips = []
        for clip in tqdm(clips, desc='Extracting skeleton data', leave=False):
            pedestrian_info = clip.reset_index().sort_values('frame')

            video_id = pedestrian_info.iloc[0]['video']
            start_frame = pedestrian_info.iloc[0]['frame']
            stop_frame = pedestrian_info.iloc[-1]['frame'] + 1

            for i, f in enumerate(range(start_frame, stop_frame, 1)):
                gt_bbox = pedestrian_info.iloc[i][[
                    'x1', 'y1', 'x2', 'y2']].to_numpy().reshape((2, 2)).astype(np.float32)
                with open(os.path.join(self.openpose_dir, video_id, '{:s}_{:0>12d}_keypoints.json'.format(
                    video_id,
                    f
                ))) as jp:
                    people = json.load(jp)['people']
                    if not len(people):
                        # OpenPose didn't detect anything in this frame - append empty array
                        pedestrian_info.at[pedestrian_info.index[i], 'keypoints'] = np.zeros(
                            (len(self.data_nodes), 3)).tolist()
                    else:
                        # select the pose with biggest IOU with base bounding box
                        candidates = [np.array(p['pose_keypoints_2d']).reshape(
                            (-1, 3)) for p in people]
                        pedestrian_info.at[pedestrian_info.index[i], 'keypoints'] = self._select_best_candidate(
                            candidates, gt_bbox).tolist()

            updated_clips.append(pedestrian_info)

        return updated_clips

    def _select_best_candidate(self, candidates: List[np.ndarray], gt_bbox: np.ndarray, iou_threshold: float = 1e-1) -> np.ndarray:
        """
        Selects the pose with the biggest overlap with ground truth bounding box.
        If the IOU is smaller than `near_zero` value, it returns empty array.

        :param candidates: [description]
        :type candidates: List[np.ndarray]
        :param gt_bbox: [description]
        :type gt_bbox: np.ndarray
        :param iou_threshold: if best IoU is lower than this thereshold, return all zeros (not detected), defaults to 1e-1
        :type iou_threshold: float, optional
        :return: Best pose candidate for specified bounding box.
        :rtype: np.ndarray
        """
        candidates_bbox = np.array([
            np.array([
                c[:, 0:2].min(axis=0),
                c[:, 0:2].max(axis=0)
            ])
            for c in candidates
        ])

        gt_min = gt_bbox.min(axis=0)
        candidates_min = candidates_bbox.min(axis=1)

        gt_max = gt_bbox.max(axis=0)
        candidates_max = candidates_bbox.max(axis=1)

        intersection_min = np.maximum(gt_min, candidates_min)
        intersection_max = np.minimum(gt_max, candidates_max)

        intersection_area = (intersection_max -
                             intersection_min + 1).prod(axis=1)
        gt_area = (gt_max - gt_min + 1).prod(axis=0)
        candidates_area = (candidates_max - candidates_min + 1).prod(axis=1)

        iou = intersection_area / \
            (gt_area + candidates_area - intersection_area)

        best_iou_idx = np.argmax(iou)

        if iou[best_iou_idx] < iou_threshold:
            return np.zeros((len(self.data_nodes), 3))

        return candidates[best_iou_idx]
